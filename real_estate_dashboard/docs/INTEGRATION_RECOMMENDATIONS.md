# API Integration Recommendations

**Date:** 2025-11-09
**Purpose:** Implementation roadmap for integrating data source APIs into the real estate dashboard

---

## Executive Summary

Based on comprehensive API research, this document provides prioritized recommendations for integrating external data sources into the real estate dashboard. The focus is on free government APIs that provide high-value real estate insights with historical data.

---

## ðŸŽ¯ Tier 1: Immediate Implementation (High Priority)

### 1. FRED API - Federal Reserve Economic Data

**Priority:** CRITICAL
**Effort:** Low (2-3 days)
**Value:** Very High
**Cost:** Free

#### Why Implement First?
- Single most comprehensive source of economic indicators
- No rate limits
- Excellent historical data (50+ years for many series)
- Direct impact on real estate market analysis
- Already partially implemented in codebase

#### Key Metrics to Integrate
```python
HOUSING_SERIES = {
    'HOUST': 'Housing Starts',
    'CSUSHPINSA': 'Case-Shiller Home Price Index',
    'MORTGAGE30US': '30-Year Mortgage Rate',
    'MSPUS': 'Median Sales Price',
    'PERMIT': 'Building Permits',
}
```

#### Implementation Steps
1. Extend existing `FREDIntegration` class in `/backend/app/integrations/market_data/fred.py`
2. Add new series IDs to configuration
3. Create database models for time series storage
4. Implement ETL pipeline with daily updates
5. Build API endpoints for frontend consumption
6. Create visualization components

#### Database Schema
```python
class FREDTimeSeries(Base):
    __tablename__ = "fred_time_series"

    id = Column(Integer, primary_key=True)
    series_id = Column(String, index=True)
    date = Column(Date, index=True)
    value = Column(Float)
    created_at = Column(DateTime, default=datetime.utcnow)

    __table_args__ = (
        UniqueConstraint('series_id', 'date', name='uix_series_date'),
    )
```

---

### 2. Census Bureau API - Demographics & Housing

**Priority:** CRITICAL
**Effort:** Medium (5-7 days)
**Value:** Very High
**Cost:** Free

#### Why Implement?
- Most authoritative demographic data source
- Granular geographic data (down to block group)
- Essential for market analysis and property valuation
- Rich housing data (values, rents, tenure, vacancy)

#### Key Variables to Integrate
```python
PRIORITY_VARIABLES = {
    # Housing
    'B25001_001E': 'Total Housing Units',
    'B25077_001E': 'Median Home Value',
    'B25064_001E': 'Median Gross Rent',
    'B25002_003E': 'Vacant Housing Units',

    # Demographics
    'B01003_001E': 'Total Population',
    'B19013_001E': 'Median Household Income',
    'B23025_005E': 'Unemployment Count',

    # Economic
    'B17001_002E': 'Population Below Poverty',
}
```

#### Implementation Steps
1. Create `CensusIntegration` class
2. Implement geography resolution (ZIP â†’ Census Tract â†’ County)
3. Build multi-year data fetching for trends
4. Create aggregation logic for different geographic levels
5. Design caching strategy (data changes annually)
6. Build comparison tools (YoY, neighborhood vs city vs nation)

#### API Endpoints to Create
```python
GET /api/v1/census/demographics/{zip_code}
GET /api/v1/census/housing/{zip_code}
GET /api/v1/census/trends/{zip_code}?years=5
GET /api/v1/census/compare/{zip_code_list}
```

---

### 3. HUD USER API - Fair Market Rents & Income Limits

**Priority:** HIGH
**Effort:** Low (2-3 days)
**Value:** High
**Cost:** Free

#### Why Implement?
- Critical for rental market analysis
- Income qualification calculations
- Affordable housing program eligibility
- Historical rent trends (40+ years)
- Already partially implemented

#### Key Endpoints
```python
ENDPOINTS = {
    'fmr': '/fmr/data/{zip}',           # Fair Market Rents
    'il': '/il/data/{state}',            # Income Limits
    'usps': '/usps',                     # ZIP crosswalk
}
```

#### Implementation Steps
1. Extend existing `HUDIntegration` class
2. Add historical FMR trend analysis
3. Implement income limit calculators
4. Create rent comparison tools
5. Build affordability index calculations

#### Use Case: Rent Analysis Dashboard
```python
class RentAnalysisTool:
    def compare_to_fmr(self, zip_code, bedrooms, proposed_rent):
        """Compare proposed rent to HUD FMR"""
        fmr = self.get_fair_market_rent(zip_code, bedrooms)
        return {
            'fmr': fmr,
            'proposed': proposed_rent,
            'percentage_of_fmr': (proposed_rent / fmr) * 100,
            'qualifies_for_section8': proposed_rent <= fmr
        }
```

---

## ðŸŽ¯ Tier 2: Near-Term Implementation (Medium Priority)

### 4. BLS API - Employment & CPI Data

**Priority:** MEDIUM
**Effort:** Medium (4-5 days)
**Value:** High
**Cost:** Free

#### Key Series
- Housing CPI (rent inflation)
- Construction employment
- Construction PPI (material costs)
- Regional employment data

#### Implementation Focus
1. Housing cost inflation tracking
2. Construction cost forecasting
3. Employment correlation with housing demand
4. Operating expense inflation

---

### 5. SEC EDGAR API - REIT & Public Company Data

**Priority:** MEDIUM
**Effort:** Medium-High (6-8 days)
**Value:** Medium-High
**Cost:** Free

#### Key Features
- REIT financial performance tracking
- Competitor analysis
- Market trend identification
- Insider trading signals

#### Implementation Focus
1. Major REIT tracking dashboard
2. Financial metrics extraction (XBRL parsing)
3. Filing alerts (new 10-K, 10-Q, 8-K)
4. Industry benchmarking

---

### 6. EPA Envirofacts API - Environmental Risk

**Priority:** MEDIUM
**Effort:** Low-Medium (3-4 days)
**Value:** Medium
**Cost:** Free

#### Key Data
- Superfund site proximity
- Environmental hazard screening
- Property due diligence support

#### Implementation Focus
1. Proximity search (sites within X miles)
2. Risk scoring algorithm
3. Due diligence report generation
4. Alert system for new listings

---

## ðŸŽ¯ Tier 3: Future Implementation

### 7. NOAA Climate API - Weather & Climate Risk

**Priority:** LOW-MEDIUM
**Effort:** Medium (5-6 days)
**Value:** Medium
**Cost:** Free

#### Use Cases
- Flood risk assessment
- Climate risk scoring
- Insurance underwriting support
- Long-term investment analysis

---

## ðŸ—ï¸ Integration Architecture

### Recommended Pattern

```python
# Base integration class (already exists)
class BaseIntegration:
    def __init__(self, config):
        self.config = config
        self.client = self._setup_client()

    def fetch_data(self):
        """Override in subclass"""
        raise NotImplementedError

    def cache_data(self, data, ttl=3600):
        """Cache with TTL"""
        pass

    def get_cached_or_fetch(self, key):
        """Check cache first"""
        pass

# Example implementation
class CensusIntegration(BaseIntegration):
    BASE_URL = "https://api.census.gov/data"

    def __init__(self, config):
        super().__init__(config)
        self.api_key = config.CENSUS_API_KEY

    def get_demographics(self, zip_code, year=2023):
        cache_key = f"census_demo_{zip_code}_{year}"

        cached = self.get_cached(cache_key)
        if cached:
            return cached

        # Fetch from API
        data = self._fetch_acs_data(zip_code, year)

        # Cache for 1 year (data doesn't change)
        self.cache_data(cache_key, data, ttl=31536000)

        return data
```

### Database Strategy

#### Time Series Data
```python
# Generic time series table for economic data
class EconomicTimeSeries(Base):
    __tablename__ = "economic_time_series"

    id = Column(Integer, primary_key=True)
    source = Column(String)  # 'FRED', 'BLS', etc.
    series_id = Column(String, index=True)
    date = Column(Date, index=True)
    value = Column(Float)
    metadata = Column(JSONB)  # Store additional context
    created_at = Column(DateTime)
```

#### Snapshot Data
```python
# Census data snapshots
class CensusSnapshot(Base):
    __tablename__ = "census_snapshots"

    id = Column(Integer, primary_key=True)
    geography_type = Column(String)  # 'zip', 'tract', 'county'
    geography_id = Column(String, index=True)
    year = Column(Integer, index=True)
    data = Column(JSONB)  # All variables in JSON
    created_at = Column(DateTime)
```

### Caching Strategy

| Data Source | Update Frequency | Cache TTL | Strategy |
|-------------|------------------|-----------|----------|
| FRED | Daily | 24 hours | Refresh daily |
| Census ACS | Annual | 1 year | Refresh yearly |
| HUD FMR | Annual | 1 year | Refresh yearly |
| BLS | Monthly | 7 days | Refresh weekly |
| SEC EDGAR | Real-time | 1 hour | Short cache |
| EPA | Quarterly | 30 days | Monthly refresh |
| NOAA | Daily | 7 days | Weekly refresh |

---

## ðŸ“Š Data Pipeline Design

### ETL Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Sources   â”‚
â”‚  (FRED, Census, â”‚
â”‚   HUD, BLS...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Extract Layer  â”‚
â”‚  - Rate limit   â”‚
â”‚  - Retry logic  â”‚
â”‚  - Error handle â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transform Layer â”‚
â”‚  - Normalize    â”‚
â”‚  - Validate     â”‚
â”‚  - Enrich       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Load Layer    â”‚
â”‚  - Database     â”‚
â”‚  - Cache        â”‚
â”‚  - Index        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Endpoints  â”‚
â”‚  - REST API     â”‚
â”‚  - WebSocket    â”‚
â”‚  - GraphQL      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scheduled Jobs

```python
# Example: Daily FRED update
@celery.task
def update_fred_series():
    fred = FREDIntegration(config)

    for series_id in TRACKED_SERIES:
        try:
            data = fred.get_latest_observation(series_id)
            store_in_database(data)
            update_cache(series_id, data)
            logger.info(f"Updated {series_id}")
        except Exception as e:
            logger.error(f"Failed to update {series_id}: {e}")
            alert_admin(series_id, e)

# Schedule
beat_schedule = {
    'update-fred-daily': {
        'task': 'tasks.update_fred_series',
        'schedule': crontab(hour=6, minute=0),  # 6 AM daily
    },
}
```

---

## ðŸ” Configuration Management

### Environment Variables

```bash
# .env.integrations
CENSUS_API_KEY=your_census_key
FRED_API_KEY=your_fred_key
HUD_API_KEY=your_hud_key
BLS_API_KEY=your_bls_key
NOAA_API_KEY=your_noaa_key

# Feature flags
ENABLE_CENSUS_INTEGRATION=true
ENABLE_FRED_INTEGRATION=true
ENABLE_HUD_INTEGRATION=true
ENABLE_BLS_INTEGRATION=false
ENABLE_SEC_EDGAR_INTEGRATION=false
ENABLE_EPA_INTEGRATION=false
ENABLE_NOAA_INTEGRATION=false
```

### Config Class

```python
class IntegrationConfig(BaseSettings):
    # API Keys
    census_api_key: Optional[str] = Field(None, env='CENSUS_API_KEY')
    fred_api_key: Optional[str] = Field(None, env='FRED_API_KEY')
    hud_api_key: Optional[str] = Field(None, env='HUD_API_KEY')

    # Feature flags
    enable_census: bool = Field(True, env='ENABLE_CENSUS_INTEGRATION')
    enable_fred: bool = Field(True, env='ENABLE_FRED_INTEGRATION')

    # Rate limits
    fred_max_requests_per_second: int = 10
    census_max_requests_per_second: int = 5

    class Config:
        env_file = '.env.integrations'
```

---

## ðŸ“ˆ Success Metrics

### Technical Metrics
- API response time < 500ms (cached)
- API response time < 2s (uncached)
- Cache hit rate > 80%
- Error rate < 1%
- Data freshness: Updated within defined TTL

### Business Metrics
- Number of data points available
- Historical depth (years of data)
- Geographic coverage (% of US)
- Feature adoption rate
- User engagement with data visualizations

---

## ðŸš¨ Risk Mitigation

### API Reliability
- **Issue:** APIs may be unavailable
- **Mitigation:**
  - Cache all data
  - Implement fallback to stale data
  - Queue failed requests for retry
  - Alert on extended outages

### Rate Limiting
- **Issue:** Exceed API rate limits
- **Mitigation:**
  - Implement request queuing
  - Respect documented limits
  - Use exponential backoff
  - Monitor usage metrics

### Data Quality
- **Issue:** Incorrect or missing data
- **Mitigation:**
  - Validate all responses
  - Implement data quality checks
  - Log anomalies
  - Manual review process

### API Changes
- **Issue:** API endpoints/schema changes
- **Mitigation:**
  - Version API clients
  - Monitor API changelog
  - Comprehensive error logging
  - Graceful degradation

---

## ðŸ“ Implementation Checklist

### Phase 1: FRED Integration (Week 1-2)
- [ ] Create `FREDIntegration` enhancement
- [ ] Define series IDs to track
- [ ] Create database models
- [ ] Implement ETL pipeline
- [ ] Build API endpoints
- [ ] Create frontend components
- [ ] Write tests
- [ ] Deploy to staging
- [ ] Monitor for 1 week
- [ ] Deploy to production

### Phase 2: Census Integration (Week 3-4)
- [ ] Create `CensusIntegration` class
- [ ] Implement variable selection
- [ ] Create database models
- [ ] Build geographic resolution logic
- [ ] Implement trend analysis
- [ ] Create comparison tools
- [ ] Write tests
- [ ] Documentation
- [ ] Deploy

### Phase 3: HUD Integration (Week 5)
- [ ] Enhance `HUDIntegration`
- [ ] Add historical FMR data
- [ ] Build rent comparison tools
- [ ] Create affordability calculators
- [ ] Write tests
- [ ] Deploy

### Phase 4: Additional Integrations (Week 6+)
- [ ] BLS API
- [ ] SEC EDGAR API
- [ ] EPA API
- [ ] NOAA API (if needed)

---

## ðŸ’¡ Best Practices

### API Client Design
1. **Use session pooling** for HTTP connections
2. **Implement automatic retries** with exponential backoff
3. **Set reasonable timeouts** (10-30 seconds)
4. **Log all API calls** for debugging
5. **Monitor API health** continuously

### Data Management
1. **Store raw responses** for audit trail
2. **Version your data schemas**
3. **Implement data validation** at ingestion
4. **Create data quality dashboards**
5. **Archive old data** appropriately

### Error Handling
1. **Never fail silently** - log everything
2. **Distinguish** between temporary and permanent failures
3. **Provide meaningful error messages** to users
4. **Implement circuit breakers** for failing APIs
5. **Alert on-call** for critical failures

---

## ðŸŽ“ Training & Documentation

### For Developers
- API integration patterns
- Database schema design
- Caching strategies
- Error handling best practices

### For Users
- Data dictionary
- Metric definitions
- Update frequencies
- Data limitations

---

## ðŸ“ž Support & Maintenance

### Ongoing Tasks
- Monitor API health
- Update API keys
- Review error logs
- Optimize slow queries
- Update documentation
- Add new data sources

### Quarterly Reviews
- Assess data quality
- Review usage metrics
- Evaluate new APIs
- Deprecate unused features
- Update roadmap

---

**Document Status:** Final Recommendations
**Last Updated:** 2025-11-09
**Next Review:** 2025-12-09
